Ruta
cd "C:\Users\djsol\Documents\MINE\Semestre_II\codigo_cirrosis"
cd "C:\Users\JONANNA\Documents\MINE\codigo_cirrosis"



ruta_dataset = "ruta/del/dataset/cirrhosis.csv"

C:\Users\JONANNA\Documents\MINE\codigo_cirrosis


C:\Users\JONANNA\AppData\Roaming\Python\Scripts\poetry init

jupyter notebook

http://localhost:8888/tree?token=1c9eb5c3edfcf7ca27cc6818ca00866df3dc9bc62b8b2f3a

cirrhosis.csv

python = "^3.10"
pandas = "^1.5.0"
numpy = "^1.23.0"
scikit-learn = "^1.1.0"
joblib = "^1.1.0"
matplotlib = "^3.5.0"
seaborn = "^0.11.2"
jupyter = "^1.0.0"

scikit-learn
pandas
numpy
joblib
matplotlib
seaborn

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import pandas as pd
from joblib import dump

# Cargar los datos
data = pd.read_csv('cirrhosis.csv')  # Asegúrate de que 'cirrhosis.csv' esté en la misma carpeta que este script

# Verificar que las columnas existen
print(data.columns)

# Definir las características numéricas y categóricas (asegúrate de que estos nombres coincidan con las columnas del CSV)
numeric_features = ['Age', 'Bilirubin', 'Cholesterol', 'Albumin', 'Copper']  # Cambia según tus columnas reales
categorical_features = ['Sex', 'Ascites', 'Hepatomegaly', 'Edema']  # Cambia según tus columnas reales

# Definir el preprocesador para las columnas numéricas y categóricas
numeric_transformer = SimpleImputer(strategy='mean')
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder())
])

# Preprocesar las características
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# Separar las características (X) y la variable objetivo (y)
X = data.drop(columns=['Stage'])  # Aquí 'Stage' es la columna objetivo
y = data['Stage']

# Dividir los datos en conjunto de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Crear el pipeline con el preprocesador y el modelo
model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Entrenar el modelo
model_pipeline.fit(X_train, y_train)

# Guardar el pipeline
dump(preprocessor, 'Código/preprocessing_pipeline.joblib')
dump(model_pipeline, 'Código/model_pipeline.joblib')

# Verificar la precisión del modelo
accuracy = model_pipeline.score(X_test, y_test)
print(f"Modelo entrenado con una precisión del {accuracy*100:.2f}%")

-----------------------------------------------------------------
Los estudiantes deben configurar un entorno de desarrollo para el proyecto, gestionando todas
las dependencias mediante poetry. Es obligatorio definir el archivo pyproject.toml.
• Realizar una exploraci´on detallada del dataset Cirrhosis Patient Survival Prediction Dataset, generando visualizaciones y describiendo los hallazgos en un cuaderno Jupyter
ubicado en la carpeta Codigo.
• Crear un pipeline de preprocesamiento que incluya manejo de valores nulos y codificacion de variables categoricas. Ampliar el pipeline para incluir un modelo de clasificaci´on.
• Guardar los pipelines (preprocesamiento y preprocesamiento + modelo) en la carpeta Codigo utilizando joblib.
• Actualizar el archivo README del proyecto para reflejar cambios realizados y documentar las etapas del desarrollo.
• Documentar brevemente el modelo desarrollado, incluyendo detalles de las t´ecnicas utilizadas y los resultados obtenidos. Guardar esta documentaci´on en la carpeta Documentacion.
• Para acceder al repositorio del proyecto, puede hacer clic en el siguiente enlace: Repositorio del Proyecto

Nota: El trabajo debe ser enviado al correo carlos.zainea@uexternado.edu.co. Recuerde adjuntar el enlace al repositorio de GitHub junto con la documentaci´on requerida. Tiempo de realizaci´on:
Los estudiantes disponen de 3 horas para completar este ejercicio.

1. Configurar el ambiente y gestionar las dependencias utilizando poetry.
2. Realizar una exploracion inicial de los datos, documentando el proceso en un cuaderno Jupyter.
3. Desarrollar un pipeline de preprocesamiento y un modelo de clasificacion.
4. Guardar los pipelines entrenados y documentar adecuadamente el proyecto.
5. Presentar el proyecto en un repositorio de GitHub, incluyendo README actualizado y documentacion detallada




-------------------------------------
CÓDIGO MUY IMPORTANTE PARA IDENTIFCAR SI LOS ARCHIVOS ESTÁN UTF-8 O EN BINAIRO

Get-ChildItem -Recurse | ForEach-Object { 
    try {
        Get-Content $_.FullName -ErrorAction Stop | Out-Null
        "$($_.FullName): Text"
    }
    catch {
        "$($_.FullName): Binary"
    }
}

-------------------------------------

